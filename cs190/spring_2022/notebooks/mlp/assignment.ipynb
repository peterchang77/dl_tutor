{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment we will train a multilayer perceptron model on the MNIST dataset, extending the linear (single-layer) classifier in the tutorial. \n",
    "\n",
    "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Once complete, the following items must be submitted:\n",
    "\n",
    "* final `*.ipynb` notebook\n",
    "* final trained `*.hdf5` model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from tensorflow.keras import Input, Model, models, layers, losses, metrics, optimizers\n",
    "from jarvis.train import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "As in the tutorial, data for this assignment will consist of the MNIST handwritten digit dataset. The following lines of code will:\n",
    "\n",
    "1. Download the dataset (if not already present) \n",
    "2. Prepare the necessary Python generators to iterate through dataset\n",
    "3. Prepare the corresponding Tensorflow Input(...) objects for model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='mnist')\n",
    "\n",
    "# --- Prepare generators and model inputs\n",
    "gen_train, _, client = datasets.prepare(name='mnist', custom_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: There is no need to change the above code for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this assignment we will train a multilayer perceptron, e.g. a simple neural network with at least one hidden layer. Be creative; feel free to try various permutations of: \n",
    "\n",
    "* number(s) of hidden layer(s)\n",
    "* size of hidden layer(s)\n",
    "* learning rate\n",
    "* training iterations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define input\n",
    "x = Input(shape=?, dtype='float32')\n",
    "\n",
    "# --- Define model\n",
    "h1 = layers.Dense(...)(x)\n",
    "# h2 = ...\n",
    "# h3 = ...\n",
    "\n",
    "logits = layers.Dense(?)(?)\n",
    "\n",
    "# --- Create model\n",
    "backbone = Model(inputs=x, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define inputs\n",
    "inputs = {\n",
    "    'dat': Input(?),\n",
    "    'digit': Input(?)}\n",
    "\n",
    "# --- Define model\n",
    "logits = backbone(inputs['dat'])\n",
    "\n",
    "# --- Define loss\n",
    "sce = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss = sce(?)\n",
    "\n",
    "# --- Define metric\n",
    "acc = metrics.sparse_categorical_accuracy(y_true=inputs['digit'], y_pred=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create the `training` model and add the corresponding loss and accuracy tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model\n",
    "training = Model(inputs=inputs, outputs=?)\n",
    "\n",
    "# --- Add loss\n",
    "training.add_loss(?)\n",
    "\n",
    "# --- Add metric\n",
    "training.add_metric(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling\n",
    "\n",
    "Once the `training` model has been created, use the following to define an optimizer and compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define optimizer \n",
    "optimizer = optimizers.Adam(learning_rate=?)\n",
    "\n",
    "# --- Compile model\n",
    "training.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now compiled and ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fit(\n",
    "    generator=gen_train, \n",
    "    steps_per_epoch=?, \n",
    "    epochs=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Based on the tutorial discussion, use the following cells to check your algorithm performance. Consider loading a saved model and running prediction using `training.predict(...)` on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate accuracy\n",
    "arrs = client.get(rows=np.arange(60000))\n",
    "pred = training.predict(...)\n",
    "\n",
    "# --- Serialize as *.csv file\n",
    "df = pd.DataFrame(index=client.db.fnames.index)\n",
    "df['true'] = ...\n",
    "df['pred'] = ...\n",
    "df['corr'] = ...\n",
    "\n",
    "# --- Print cumulative model perforance\n",
    "df['corr'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this cell is used only to check for model performance. It will not be graded. Once you are satisfied with your model, proceed to submission of your assignment below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Use the following line to save your model for submission (in Google Colab this should save your model file into your personal Google Drive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "fname = './final.hdf5'\n",
    "backbone.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canvas\n",
    "\n",
    "Once you have completed this assignment, download the necessary files from Google Colab and your Google Drive. You will then need to submit the following items:\n",
    "\n",
    "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
    "* final (trained) model: `[UCInetID]_model.hdf5`\n",
    "\n",
    "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooked file would be submitted under the name `panteater_notebook.ipynb` and his model file would be submitted under the name `panteater_model.hdf5`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
