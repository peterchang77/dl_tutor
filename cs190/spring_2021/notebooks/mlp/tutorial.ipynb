{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will introduce key concepts related to creating simple neural networks including a linear (single-layer) classifier as well as a multilayer perceptron.\n",
    "\n",
    "**Linear Classifiers and Multilayer Perceptrons**\n",
    "\n",
    "* matrix multiplication\n",
    "* softmax cross-entroy loss\n",
    "* model training\n",
    "* evaluation\n",
    "* visualization\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "%pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, models, layers\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of the MNIST handwritten digit dataset. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/mnist`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, _, client = datasets.prepare(name='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the generator yields a batch size of 128 different images. As before, each iteration yields two variables, `xs` and `ys`, each representing a dictionary of model input(s) and output(s). In the current example, there is just a single input and output. Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('ys shape: {}'.format(ys['digit'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 28 x 28 2D image is serialized as a 784-element vector. Use the following lines of code to reshape the data vector and visualize using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reshape to 2D images\n",
    "dat = xs['dat'].reshape(-1, 28, 28)\n",
    "\n",
    "# --- Show the first example\n",
    "imshow(dat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the entire dat array to `imshow(...)` to display an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of first 64 images\n",
    "imshow(dat[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the 64-element `ys['digit']` vector corresponds to ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print ys['digit']\n",
    "print(ys['digit'][:64].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations\n",
    "\n",
    "As a review, recall the diagramatic representation of a neural network covered in lecture:\n",
    "\n",
    "![Diagramtric Representation](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/mlp/pngs/nn_diagram.png)\n",
    "\n",
    "Recall that each reach \"retinal neuron\" recieving light from the image is connected to one of ten output neurons, each indicating the relative likelihood of that image representing any one of the ten possible digits. The neuron with the largest score value (e.g. **logit score**) in turn represents the most probable interpretation by the network of the provided digit image.\n",
    "\n",
    "Also, recall that each connection itself is modeled by a multiplicative weight value that represents how strong (or weak) the connection is, and that each of the 784 connections to one of our ten output neurons can in fact be represented as a matrix of numbers:\n",
    "\n",
    "![Matrix Representation](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/mlp/pngs/nn_multiply.png)\n",
    "\n",
    "Of course, for each of our ten digits, we will have a different 28 x 28 weight matrix for a total of 10 weight matrices. Finally keep in mind that for the sake of ease in representation (given our simple model) both our 28 x 28 input matrix and ten 28 x 28 weight matrices can be **flattened** to just a single 1 x 784 (or 784 x 1) matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "As in the earlier examples, the first dimension of the input matrix `x` will be `None` to indicate that one or many images can be passed through the model at a time. The second dimension of the input matrix will need to accomodate a **flattened** version of our 28 x 28 matrix:\n",
    "\n",
    "```\n",
    "x = [[x00, x01, x02, ...],   ==> (flattening) ==> [[x00, x01, x02, x10, x11, ..., xNN]]\n",
    "     [x10, x11, x12, ...],\n",
    "     [x20, x21, x22, ...]\n",
    "     ... , ..., ..., xNN]]\n",
    "```\n",
    "\n",
    "Thus our input matrix will need to be `(None, 28 x 28)` = `(None, 784)`. Use the following code cell to create our `Input(...)` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Tensorflow input\n",
    "x = Input(shape=(784,))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the data generator above yields a dictionary of possible `xs` inputs. To accomodate for this potential of multi-input models, the Tensorflow 2.0 / Keras simply specifies that each defined `Input()` object be maintained in a dictionary with matching keys and names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Tensorflow input dict\n",
    "inputs = {}\n",
    "inputs['dat'] = Input(shape=(784,), name='dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### client.get_inputs(...)\n",
    "\n",
    "Alternatively, instead of manually creating each potential model input, the `client` object as part of the Jarvis library has a convenience method to automatically create matching input(s) for all the input arrays yielded as part of `xs`. To use, simply pass the Tensorflow `Input` class to the `client.get_inputs(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model inputs\n",
    "inputs = client.get_inputs(Input)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs['dat'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the proper matrix multiplication operation to implement mapping of input (image) to output (digit logit scores). Recall the earlier discussion on **dense** layers and matrix multiplication:\n",
    "\n",
    "```\n",
    "Let:\n",
    "\n",
    "X = 1 x N matrix = [a, b, c, d, e...]\n",
    "\n",
    "M = N x 1 matrix = [[A],\n",
    "                    [B],\n",
    "                    [C],\n",
    "                    [D],\n",
    "                    [E],\n",
    "                    ...\n",
    "                    ]\n",
    "\n",
    "Then:\n",
    "\n",
    "X x M = ...\n",
    "\n",
    "= [[a, b, c, d, e...]] x [[A], = a * A + b * B + c * C + d * D + e * E ...\n",
    "                          [B],\n",
    "                          [C],\n",
    "                          [D],\n",
    "                          [E],\n",
    "                          ...\n",
    "                          ]\n",
    "                          \n",
    "```\n",
    "\n",
    "The following rules extend this formally to 2D matrices.\n",
    "\n",
    "```\n",
    "Let:\n",
    "\n",
    "X = I x N matrix\n",
    "M = N x J matrix\n",
    "```\n",
    "\n",
    "1. The **second** dimension of `X` must match the **first** dimension of `M` (e.g. `N`).\n",
    "2. The output of the matrix multiplication is equal to the **first** dimension of `X` by the **second** dimension of `Y` (e.g. `I x J`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the discussion above, in order to define a *matrix multiplication* between the input Tensor and the `m` weights matrix, what shape must the `m` weights matrix consist of to yield 10 different logit score predictions?\n",
    "\n",
    "Use the following code cell to create the appropriate operation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create op\n",
    "op = layers.Dense(10)\n",
    "\n",
    "# --- Create output\n",
    "y = op(inputs['dat'])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check to see if the weights are of correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get weight matrix shape\n",
    "w, b = op.get_weights()\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "Just as the Generator input `xs` is defined as a dictionary of one or multiple model inputs, the Generator output `ys` is defined as a dictionary of one or multiple model outputs. To accomodate for this, the Tensorflow 2.0 / Keras API simply specifies that each output logit score (defined via a `Tensor` object) is maintained in a dictionary. Just like inputs, the logit score object(s) must also have matching names where are defined in the **final operation** that yields the logit score `Tensor` object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create op (note that the output Tensor name must also be defined here)\n",
    "op = layers.Dense(10, name='digit')\n",
    "\n",
    "# --- Create outputs\n",
    "logits = {}\n",
    "logits['digit'] = op(inputs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "This single operation model successfully defines the linear classification model as presented during lecture. Use the following line of code to create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following line of code to pass a single batch of data through the created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict\n",
    "scores = model.predict(xs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the output shape of the `scores` variable, what does this variable represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logit scores\n",
    "print(scores['digit'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the Model\n",
    "\n",
    "### Softmax cross-entropy loss\n",
    "\n",
    "After generating **logit** scores, one must define a method to: (1) convert logit scores to final prediction; (2) quantify the *goodness* of model prediction. These steps are performed mathematically with what is known as a loss function. For classification problems, the standard function to use is the **softmax cross-entropy** loss. This loss in fact is composed of two seperate steps:\n",
    "\n",
    "* **softmax**: method to normalize logit scores into a probability distribution\n",
    "* **cross-entropy**: method to quantify difference between predicted distribution and true distribution\n",
    "\n",
    "To define a softmax cross-entropy loss in Tensorflow, use the `losses.SparseCategoricalCrossentropy(...)` class. The sparse formulation is a special case of cross-entropy loss where only a single mutually exclusive class is correct for any given input. The `from_logits=True` argument is used to denote that raw logit scores are passed as input and that a softmax normalization is required before calculating cross-entropy loss. It is recommended to perform these two steps simultaneously rather than split into two seperate operations for numeric stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a loss object\n",
    "sce = losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, use the following lines of code to calculate the loss for the generate logit scores of the current data batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate loss\n",
    "sce(y_true=ys['digit'], y_pred=scores['digit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training proceeds, the goal of the algorithm is to update weights in a manner such that this loss value decreases over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define an Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Putting everything together, use the following cell to define, create and compile the single-layer linear classifier model for the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model inputs\n",
    "inputs = client.get_inputs(Input)\n",
    "\n",
    "# --- Define model\n",
    "logits = {}\n",
    "logits['digit'] = layers.Dense(10, name='digit')(inputs['dat'])\n",
    "\n",
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'digit': sce},\n",
    "    metrics={'digit': 'sparse_categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that given the dictionary of potential output(s), each of which may have it's own separate loss function and/or accuracy metric:\n",
    "\n",
    "* loss must represent a dictionary of loss objects with keys matching `ys`\n",
    "* metrics must represent a dictionary of metrics with keys matching `ys`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Once the model has been compiled and the data prepared (via a generator), training can be invoked using the `model.fit(...)` method. The only parameters that need to be set relate to the number of total training iterations (total number of steps = `steps_per_epoch` x `epochs`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=250, \n",
    "    epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `model.predict(...)` to obtain logit scores\n",
    "* use `np.argmax(...)` to obtain prediction\n",
    "* compare prediction with ground-truth\n",
    "* serialize in Pandas DataFrame\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, use the `client.get(rows=...)` method. Since there is no special train or valid split currently, simply load all 60,000 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load all data\n",
    "arrs = client.get(rows=np.arange(60000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to run prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict\n",
    "scores = model.predict(arrs['xs']['dat'])\n",
    "\n",
    "# --- Argmax\n",
    "pred = np.argmax(scores['digit'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare results in Pandas DataFrame for ease of analysis and sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=np.arange(60000))\n",
    "\n",
    "# --- Define columns\n",
    "df['true'] = arrs['ys']['digit'][:, 0]\n",
    "df['pred'] = pred\n",
    "df['corr'] = df['true'] == df['pred']\n",
    "\n",
    "# --- Print accuracy\n",
    "print(df['corr'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "What are the expected weights learned by the model? Use the following lines of code to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get weights of first layer\n",
    "weights = model.layers[1].get_weights()[0]\n",
    "\n",
    "# --- Show\n",
    "imshow(weights.T.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "model.save('./mlp.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model('./mlp.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The following exercise questions will assist in preparation for this lesson's assignment, to extend our single-layer linear classifier into a multilayer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "The key to synthesizing multiple layers is the addition of activation functions after each matrix multiplication. How do you add a **relu** activation to the `layers.Dense(...)` call? \n",
    "\n",
    "Use the following cell to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = layers.Dense(10, ?, name='digit')(inputs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints\n",
    "\n",
    "Use the Jupyter / iPython `?` command to see function and class initialization signatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "All hidden layers require an activation function, however the last layer (e.g. the layer that yields the final logit scores) **does not**. Why is this so?\n",
    "\n",
    "Create a single hidden layer neural network below, ensuring that the last layer does not have any activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define hidden layer size\n",
    "hidden_size = ?\n",
    "\n",
    "# --- Define network\n",
    "hidden = layers.Dense(?, activation=?)(inputs['dat'])\n",
    "logits = layers.Dense(10, activation=?, name='digit')(hidden) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints\n",
    "\n",
    "If defined correctly, the model **logit scores** should return values that are both negative and positive. If the `ReLU` activation was accidently applied to the last layer, the logit scores should only be positive. How do we check for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create the model\n",
    "model = Model(inputs=?, outputs=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pass an example xs input into the model\n",
    "xs, ys = next(gen_train)\n",
    "scores = model.predict(?)\n",
    "\n",
    "# --- Print scores\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
