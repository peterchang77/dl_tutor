{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will explore best practices for building modern CNN models, focusing on four key design components: layer-level (micro-)architecture, model size and topology, loss functions, and block-level (macro-)architecture. For each design component, we perform a grid search to characterize the effect of each choice on overall model performance. By systematically progressing through each hyperparameter configuration, we attempt to derive general recommendations and best approaches for empiric experimentation. As a representative use case, we will build various convolutional neural networks (CNNs) for segmentation of renal tumors from CT abdomen scans from the KiTS 2021 Challenge. \n",
    "\n",
    "## Workshop Links\n",
    "\n",
    "This tutorial focuses on specific considerations related network architecture and hyperparameter tuning. Other useful tutorials can be found at this link: https://github.com/peterchang77/dl_tutor/tree/master/workshops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Environment\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install Jarvis library\n",
    "%pip install tensorflow==2.14.0 \n",
    "%pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, layers, optimizers, losses, callbacks\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of kidney tumor CT exams derived from the Kidney Tumor Segmentation Challenge (KiTS). More information about he KiTS Challenge can be found here: https://kits21.kits-challenge.org/. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/ct_kits`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='ct/kits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality. As needed, pass any custom configurations (e.g. batch size, normalization parameters, etc) into the optional `configs` dictionary argument. \n",
    "\n",
    "To specificy the correct Generator template file, pass a designated `keyword` string. In this tutorial, we will be using abdominal CT volumes that have been preprocessed into 96 x 96 x 96 matrix volumes, each cropped to the right and left kidney, facilitating ease of algorithm training within the Google Colab platform. To select the correct Client template for this task, use the keyword string `3d-pos`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='3d-pos', configs={'batch': {'size': 2}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created generators yield a total of `batch['size']` training samples based on the specified batch size. Each iteration yields a dictionary of model inputs, `data`. In the current example, there is just a single input volume `data['x']` and a single target `data['y']`. Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "data, _ = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "for k, v in data.items():\n",
    "    print('key = {} : shape = {}'.format(k.ljust(7), v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITS Data\n",
    "\n",
    "As noted above, the input images in the variable `data['x']` are matrices of shape `96 x 96 x 96 x 1` cropped to the right and left kidneys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example\n",
    "imshow(data['x'][0], vm=[-256, +256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `montage(...)` function to create an N x N mosaic of all middle slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images\n",
    "imshow(data['x'][:, 48], vm=[-256, +256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney masks\n",
    "\n",
    "The ground-truth labels are binary masks of the same matrix shape as the model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['y'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `imshow(...)` method to visualize the ground-truth tumor mask labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show tumor masks overlaid on original data\n",
    "imshow(data['x'][:, 48], data['y'][:, 48], vm=[-256, +256])\n",
    "\n",
    "# --- Show tumor masks isolated\n",
    "imshow(data['y'][:, 48])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design\n",
    "\n",
    "This tutorial focuses on the following key components of modern neural network model design:\n",
    "\n",
    "* Layer-level (Micro-)Architecture\n",
    "* Model Size and Topology\n",
    "* Loss Function\n",
    "* Block-level (Macro-)Architecture\n",
    "\n",
    "While there are are certainly numerous additional key considerations, a deliberate and organized approach to these four primary design components captures the majority of performance variance for most tasks, enabling a customized high-performing state-of-the-art model for nearly every use case. Let us take a closer look into each of these components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-level (Micro-)Architecture\n",
    "\n",
    "A standard convolutional neural network (CNN) block is typically composed of the following three layers:\n",
    "\n",
    "* linear operation\n",
    "* feature normalization\n",
    "* nonlinear activation\n",
    "\n",
    "**Linear operation**: A convolution (or convolution transpose) layer remains a high-performing popular choice for medical imaging applications. If relevant, a 3D kernel is preferred (see Other Design Considerations below for data preparation recommendations). For the most part, a 3x3(x3) kernel size is near optimal (VGG-style) although depthwise (and/or separable) convolutions (ConvNeXt-style) do tend to yield marginal but robust improvements.\n",
    "\n",
    "**Feature normalization**: While classic approaches utilize the `BatchNormalization` method, more recent data suggests that batch-independent normalization schemes such as `GroupNormalization` or `LayerNormalization` tend to lead to more stable optimization and improved generalization. This is especially true when batch-level statistics are unstable due to high data variance (e.g., wide dynamic range as seen in medical imaging). \n",
    "\n",
    "**Nonlinear activation**: While classic approaches utilize the `ReLU` nonlinearity, the resulting sparse activations may lead to unstable gradients during optimization. While modern optimizers tend to mitigate this theoretical risk, simple replacements such as the `LeakyReLU` offer a good alternative with only minimal incremental computational cost. Of note, more complex nonlinear functions such as `eLU` and `GeLU` have shown potential marginal improvement when used in combination with recent advanced architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation\n",
    "\n",
    "The following modular template can be used to create VGG-style blocks using interchangble layer components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers(kernel_size=3, norm_name='LayerNormalization', func_name='LeakyReLU', **kwargs):\n",
    "\n",
    "    # --- Define kwargs \n",
    "    kwargs_ = {\n",
    "        'kernel_size': kernel_size,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_normal'}\n",
    "\n",
    "    # --- Define layers \n",
    "    conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs_)(x)\n",
    "    tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs_)(x)\n",
    "\n",
    "    norm = lambda x : getattr(layers, norm_name)()(x)\n",
    "    relu = lambda x : getattr(layers, func_name)()(x) \n",
    "\n",
    "    concat = lambda *x : layers.Concatenate()(list(x))\n",
    "\n",
    "    return conv, tran, norm, relu, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks_vgg(**kwargs):\n",
    "\n",
    "    conv, tran, norm, relu, concat = create_layers(**kwargs)\n",
    "\n",
    "    conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=2)))\n",
    "    tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=2)))\n",
    "\n",
    "    return conv1, conv2, tran2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Results\n",
    "\n",
    "In this section, the base network architecture is implemented using a standard VGG-style block, with each block representing the serial application of a linear operation, feature normalization, and nonlinear activation. In this context, we perform the following hyperparameter grid search:\n",
    "\n",
    "Feature normalization: \n",
    "\n",
    "* `BatchNormalization` \n",
    "* `LayerNormalization`\n",
    "\n",
    "Nonlinear activation: \n",
    "\n",
    "* `ReLU`\n",
    "* `LeakyReLU`\n",
    "\n",
    "See __[link](https://docs.google.com/viewer?url=https://raw.githubusercontent.com/peterchang77/dl_tutor/master/workshops/model-design/pdfs/00_micro.pdf)__ for summary of results.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "In general, recommend using a combination of `LayerNormalization` and `LeakyReLU` in conjunction with 3D convolution operations. As needed, consider a hyperparameter sweep of depthwise convolutions and GeLU nonlinearities (see ConvNeXt architecture below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size and Topology\n",
    "\n",
    "For any given design strategy, overall model size can often be scaled by varying either the total number of network layers or the total number of features (channel depth) per layer. By contrast, note that the total number of feature map resolutions (dictated by the total number of downsampling and, if relevant, upsampling operations) is commonly fixed such that the smallest feature map size is between 3x3(x3) to 5x5(x5). As an important consideration detailed below, the high-dimensional nature of medical imaging data makes it such that GPU memory is primarily constrained by the size of feature map activations *not* the actual model weights. \n",
    "\n",
    "**Number of layers**: At minimum, a single layer is required for each target feature map resolution. In this minimal use case, each convolutional block will include a downsampling mechanism (e.g., a series of stride-2 convolutional blocks). However, with the use of relatively small kernel sizes, it is recommended that a minimum of two convolution layers are applied at each resolution for adequate spatial coverage (e.g., alternating stride-2 and stride-1 convolutions). As an exception, the first (full resolution) feature map may sometimes be implemented with just a single layer. This aggressive downsampling strategy is typical of many state-of-the-art models with the assumption that high-frequency (small) features are not critical for high performance. In addition, specific to high-dimensional medical imaging, a disproportionate amount of GPU memory is required to instantiate full resolution feature maps. However, given the heterogeneity of medical imaging problems, this assumption should be evaluated empirically.\n",
    "\n",
    "**Feature map depth**: As noted above, full resolution feature maps account for a disproportionate amount of GPU memory when working with high-dimensional medical imaging. With respect to this consideration, relatively small feature map depths are favored in the earlier model layers, especially when building fully-convolutional encoder-decoder (U-Net) architectures. By contrast, relatively large feature maps can be used in the deeper network layers without significant incremental computational burden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation\n",
    "\n",
    "The following code snippet example can be used to define a series of alternating stride-2 and stride-1 convolutional blocks with variable feature map depth. Note that this specific implementation corresponds to the `small` topology and `linear` feature map growth rate decribed below. For full implementation, see additional code in the sections below.\n",
    "\n",
    "```python\n",
    "# --- Define filters\n",
    "filters = [16, 32, 48, 64, 80, 96]\n",
    "\n",
    "# --- Create encoder\n",
    "l0 = conv1(filters[0], x)\n",
    "l1 = conv1(filters[1], conv2(filters[1], l0))\n",
    "l2 = conv1(filters[2], conv2(filters[2], l1))\n",
    "l3 = conv1(filters[3], conv2(filters[3], l2))\n",
    "l4 = conv1(filters[4], conv2(filters[4], l3))\n",
    "l5 = conv1(filters[5], conv2(filters[5], l4))\n",
    "\n",
    "# --- Create decoder\n",
    "t4 = conv1(filters[4], tran2(filters[4], l5) + l4)\n",
    "t3 = conv1(filters[3], tran2(filters[3], t4) + l3)\n",
    "t2 = conv1(filters[2], tran2(filters[2], t3) + l2)\n",
    "t1 = conv1(filters[1], tran2(filters[1], t2) + l1)\n",
    "t0 = conv1(filters[0], tran2(filters[0], t1) + l0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Results\n",
    "\n",
    "In this section, the base network architecture is implemented using standard VGG-style blocks with `LayerNormalization` and `LeakyReLU` nonlinearity. The original `(96, 96, 96)` input volume is downsampled five times yielding a total of 6 feature map resolutions (96, 48, 24, 12, 6, and 3). At each of the 6 feature map resolutions we perform the following hyperparameter grid search:\n",
    "\n",
    "Number of layers (at each resolution):\n",
    "\n",
    "* Small: `1-2-2-2-2-2`\n",
    "* Medium: `2-2-2-2-2-2`\n",
    "* Large: `2-3-3-3-3-3`\n",
    "\n",
    "Feature map depth (at each resolution):\n",
    "\n",
    "* Linear: `16-32-48-64-80-96`\n",
    "* Exponential: `16-32-64-128-196-256`\n",
    "\n",
    "See __[link](https://docs.google.com/viewer?url=https://raw.githubusercontent.com/peterchang77/dl_tutor/master/workshops/model-design/pdfs/01_scale.pdf)__ for summary of results.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "With sufficient training data size (and the adequate use of data augmentation), larger and deeper models show a trend towards better performance over smaller and shallower models and, at minimum, tend to be non-inferior. In the absence of a rigorous grid search, recommend using the `large` model topology with `exponential` feature map growth at each resolution.  \n",
    "\n",
    "## Loss Function\n",
    "\n",
    "Of the various model design components, perhaps the most important consideration is the combination of loss functions used for optimization. In the context of the medical imaging, high performance often requires accounting for the inherent class imbalance of data distributions, both on a per-exam (rare incidence of disease) and per-voxel (small size of finding) basis. To address class imbalance in classification and segmentation tasks, standard cross-entropy loss may be complimented with the use of class weights, focal loss, and/or differentiable Dice score loss.\n",
    "\n",
    "**Class weights**: After calculating the standard per instance cross entropy loss and prior to loss reduction or aggregation, the individual loss values of imbalanced classes can be multiplied by a scalar value > 1.0 to increase the contribution of those specific exams and/or regions. For semantic segmentation tasks, this is implemented as a per-voxel weight tensor that is multiplied (point-wise) by the standard cross-entropy loss prior to reduction. Note that class weights do *not* need to fully compensate for the degree of class imbalance inherent to the data; rather a relatively small range of class weights between `[1, 10]` (and up to 20-40 in certain use cases) is typically more than adequate.\n",
    "\n",
    "**Focal loss**: Compared to manual class weights which uniformly increase the contribution of certain classes to the loss function, focal loss dynamically weighs the contribution of each instance proportional to the *difficulty* of classification. Here, the proxy for *difficulty* is defined using the loss value itself, with high loss values suggesting poor classification. The *gamma* hyperparameter defines the relative emphasis on difficult cases, with higher *gamma* values placing greater emphasis on challenging predictions. The default *gamma* value of 2 often yields robust performance, although a grid search across values between `[1, 3]` may be used to fine-tune model accuracy.\n",
    "\n",
    "**Dice loss**: The numerator of the Dice score coefficient is defined based on the sensitivity of foreground detection and is thus a strong regularizer for class imbalanced segmentation tasks. To covert the standard discrete Dice score coefficient to a continuous, differentiable function, replace the binarized prediction masks with a sigmoid-activated (binary) or softmax-activated (multi-class) logit score. Used in isolation, a Dice score loss may lead to unstable optimization dynamics (especially for small target predictions) and is thus often paired with a more stable standard cross-entropy or focal loss objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation\n",
    "\n",
    "The following code implements weighted cross-entropy loss, focal loss, and differentiable soft Dice score loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bce(y_true, y_pred, w_samp=None, from_logits=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create weighted cross-entropy loss \n",
    "    \n",
    "    \"\"\"\n",
    "    return losses.BinaryCrossentropy(from_logits=from_logits, **kwargs)(\n",
    "        y_true=y_true, \n",
    "        y_pred=y_pred, \n",
    "        sample_weight=w_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_foc(y_true, y_pred, gamma=2.0, alpha=1.0, w_samp=1.0, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create focal loss \n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Calculate standard cross entropy with alpha weighting\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        labels=y_true, logits=y_pred, pos_weight=alpha)\n",
    "\n",
    "    # --- Calculate modulation to pos and neg labels \n",
    "    p = tf.math.sigmoid(y_pred)\n",
    "    modulation_pos = (1 - p) ** gamma\n",
    "    modulation_neg = p ** gamma\n",
    "\n",
    "    mask = tf.dtypes.cast(y_true > 0.5, dtype=tf.bool)\n",
    "    modulation = tf.where(mask, modulation_pos, modulation_neg)\n",
    "\n",
    "    return tf.math.reduce_mean(modulation * loss * w_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sft(y_true, y_pred, axis=(0, 1, 2, 3), epsilon=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create soft Dice score loss \n",
    "    \n",
    "    \"\"\"\n",
    "    true = y_true[..., 0]\n",
    "    pred = tf.nn.sigmoid(y_pred[..., 0])\n",
    "\n",
    "    A = tf.math.reduce_sum(true * pred, axis=axis) * 2\n",
    "    B = tf.math.reduce_sum(true, axis=axis) + tf.math.reduce_sum(pred, axis=axis) + epsilon\n",
    "\n",
    "    return -tf.math.reduce_mean(A / B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the following code implements a regular (discrete) Dice score error metric for tracking performing during model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dsc(y_true, y_pred, axis=(1, 2, 3), **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create Dice score error \n",
    "    \n",
    "    \"\"\"\n",
    "    y_true = y_true > 0\n",
    "    y_pred = y_pred > 0\n",
    "\n",
    "    A = tf.math.count_nonzero(y_true & y_pred, axis=axis) * 2\n",
    "    B = tf.math.count_nonzero(y_true, axis=axis) + tf.math.count_nonzero(y_pred, axis=axis)\n",
    "\n",
    "    return tf.math.reduce_mean(tf.math.divide_no_nan(\n",
    "        tf.cast(A, tf.float32), \n",
    "        tf.cast(B, tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Results\n",
    "\n",
    "In this section, the base network architecture is implemented using standard VGG-style blocks with `large` model topology and `exponential` feature map growth. Subsequently, we perform the following hyperparameter grid search:\n",
    "\n",
    "* Binary cross-entropy (BCE): with class weights ranging from `[1, 2, 5, 10]`\n",
    "* Focal loss: with *gamma* values ranging from `[1.5, 2.0, 3.0]`\n",
    "* Soft Dice score loss: paired with each permutation of BCE or focal loss, as above\n",
    "\n",
    "See __[link](https://docs.google.com/viewer?url=https://raw.githubusercontent.com/peterchang77/dl_tutor/master/workshops/model-design/pdfs/02_loss.pdf)__ for summary of results.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "In general, recommend using a combination of focal loss with *gamma* value of 2.0 and soft Dice score loss for robust performance across various tasks. However, given the importance of loss function choice for optimal performance, a hyperparameter sweep of loss formulations often yields favorable compute-to-accuracy ratio gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-level (Micro-)Architecture\n",
    "\n",
    "While standard VGG-style blocks are both simple and high-performing across various medical imaging tasks, newer modeling strategies may yield marginal gains. Perhaps the most popular modification is the use of ResNet-style residual connections. More recently, the ConvNeXt-style block is a powerful motif that compiles various state-of-the art advances in a single high-performing heuristic. \n",
    "\n",
    "**ResNet**: The use of the residual connections improves overall gradient stability during the optimization process, allowing for faster and more robust model convergence as well as the potential for very deep network topologies spanning hundreds of layers. In addition, this strategy is simple to add alongside the standard VGG-style block and is thus a popular motif across various tasks.\n",
    "\n",
    "**ConvNeXt**: By compiling and testing various model improvements introduced in recent years, the authors of ConvNeXt highlight key high-performing motifs. Perhaps the most important modification is factorization of the standard convolution into independent spatial (depthwise convolution) and feature (pointwise convolution) components, significantly reducing the overall number of model weights. In addition, empiric results inspired by advances in Transformer architecture alternate the use of GeLU nonlinearities with layer normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation\n",
    "\n",
    "The following code implements ResNet-style and ConvNeXt-style blocks. Notice that by design, each block can be used interchangably with the VGG-style blocks defined earlier in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks_resnet(**kwargs):\n",
    "\n",
    "    conv1_, conv2_, tran2_ = create_blocks_vgg(**kwargs)\n",
    "    conv, tran, norm, _, _ = create_layers(kernel_size=2, **kwargs)\n",
    "\n",
    "    conv1 = lambda filters, x : conv1_(filters, conv1_(filters, x)) + x\n",
    "    conv2 = lambda filters, x : conv1_(filters, conv2_(filters, x)) + conv(x=x, filters=filters, strides=2)\n",
    "    tran2 = lambda filters, x : conv1_(filters, tran2_(filters, x)) + tran(x=x, filters=filters, strides=2)\n",
    "\n",
    "    return conv1, conv2, tran2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks_convnext(**kwargs):\n",
    "\n",
    "    conv, tran, norm, _, _ = create_layers(kernel_size=2, **kwargs)\n",
    "\n",
    "    conv1 = lambda filters, x : create_blocks_convnext_conv1(x=x, filters=filters) + x\n",
    "    conv2 = lambda filters, x : conv(norm(x), filters, strides=2)\n",
    "    tran2 = lambda filters, x : tran(norm(x), filters, strides=2)\n",
    "\n",
    "    return conv1, conv2, tran2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks_convnext_conv1(x, filters, kernel_size=3, factor=2, name=None, configs=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create ConvNext-style block\n",
    "\n",
    "    NOTE: compared to original implementation, an extra Layer Norm is added at beginning\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Define layers\n",
    "    conv = lambda x, **kwargs : layers.Conv3D(\n",
    "        kernel_initializer='he_normal', padding='same', **kwargs)(x)\n",
    "\n",
    "    norm = lambda x : layers.LayerNormalization()(x)\n",
    "    gelu = lambda x : layers.Activation('gelu')(x)\n",
    "\n",
    "    # --- Apply ConvNext block\n",
    "    l0 = conv(x=norm(x), filters=x.shape[-1], groups=x.shape[-1], kernel_size=kernel_size)\n",
    "    l1 = conv(x=norm(l0), filters=int(filters * factor), kernel_size=1)\n",
    "    l2 = conv(x=gelu(l1), filters=filters, kernel_size=1, name=name)\n",
    "\n",
    "    return l2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Results\n",
    "\n",
    "In this section, the base network architecture is implemented using standard VGG-style blocks with `large` model topology and `exponential` feature map growth. This base model comprising of standard VGG-style blocks is compared to models implemented with ResNet-style blocks or ConvNeXt-style blocks, while holding all other design choices and network topology considerations constant. Models are trained using weighted binary cross-entropy (class weight of 10) or focal loss (gamma of 2.0), either without or with a soft Dice score loss component.\n",
    "\n",
    "See __[link](https://docs.google.com/viewer?url=https://raw.githubusercontent.com/peterchang77/dl_tutor/master/workshops/model-design/pdfs/03_macro.pdf)__ for summary of results.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "Both ResNet and ConvNeXt models yield marginal but robust gains over the base VGG architecture, with a trend towards overall more consistent improvements by the ConvNeXt strategy. In the absence of a rigorous grid search, recommend a baseline VGG model as well as a ConvNeXt model using previously identified optimal configurations for other hyperparameters to assess for incremental performance improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Design Considerations\n",
    "\n",
    "Though the following design considerations have a theoretical range of valid operating choices, base assumptions and other predefined heuristics can often achieve satisfactory performance without the need for rigorous hyperparameter search.  \n",
    "\n",
    "**Data preprocessing**: In general, increased data context leads to better performance (e.g., larger field-of-view, higher resolution, 3D instead of 2D) and therefore recommend using the largest allowable model input within the constraints of available GPU memory and other model design choices noted above. As needed, consider a two-step cascaded approach to combine global context with higher resolution local features.  \n",
    "\n",
    "**Optimization**: In general, deep supervision with auxiliary loss objectives at each available feature map resolution will improve performance across all tasks. Additionally, robust performance can often be achieved with the Adam optimizer coupled with a learning rate decay, which together supports a wide range of independently optimized per-weight updates without the need for carefully calibrating the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = callbacks.LearningRateScheduler(lambda epoch, lr : lr * 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following modular code template to create models spanning the various hyperparameter configurations described in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(shape=(96, 96, 96, 1), style='vgg', losses=['bce'], class_weight=1.0, filters=[16, 32, 48, 64, 80, 96], **kwargs):\n",
    "\n",
    "    inputs = {\n",
    "        'x': Input(shape=shape, dtype='float32'),\n",
    "        'y': Input(shape=shape, dtype='float32')}\n",
    "\n",
    "    # --- Create layers\n",
    "    conv, tran, norm, relu, concat = create_layers(**kwargs)\n",
    "\n",
    "    # --- Create blocks \n",
    "    conv1, conv2, tran2 = {\n",
    "        'vgg': create_blocks_vgg,\n",
    "        'resnet': create_blocks_resnet,\n",
    "        'convnext': create_blocks_convnext}[style](**kwargs)\n",
    "\n",
    "    # --- Create first layer conv\n",
    "    if style in ['resnet', 'convnext']:\n",
    "        x = conv(inputs['x'], filters=filters[0], strides=1)\n",
    "    else:\n",
    "        x = inputs['x']\n",
    "\n",
    "    # --- Create encoder\n",
    "    l0 = conv1(filters[0], x)\n",
    "    l1 = conv1(filters[1], conv2(filters[1], l0))\n",
    "    l2 = conv1(filters[2], conv2(filters[2], l1))\n",
    "    l3 = conv1(filters[3], conv2(filters[3], l2))\n",
    "    l4 = conv1(filters[4], conv2(filters[4], l3))\n",
    "    l5 = conv1(filters[5], conv2(filters[5], l4))\n",
    "\n",
    "    # --- Create decoder\n",
    "    t4 = conv1(filters[4], tran2(filters[4], l5) + l4)\n",
    "    t3 = conv1(filters[3], tran2(filters[3], t4) + l3)\n",
    "    t2 = conv1(filters[2], tran2(filters[2], t3) + l2)\n",
    "    t1 = conv1(filters[1], tran2(filters[1], t2) + l1)\n",
    "    t0 = conv1(filters[0], tran2(filters[0], t1) + l0)\n",
    "\n",
    "    # --- Create logits\n",
    "    logits = {}\n",
    "    if style == 'convnext':\n",
    "        logits['t0'] = conv(norm(t0), filters=1, strides=1)\n",
    "        logits['t1'] = conv(norm(t1), filters=1, strides=1)\n",
    "        logits['t2'] = conv(norm(t2), filters=1, strides=1)\n",
    "        logits['t3'] = conv(norm(t3), filters=1, strides=1)\n",
    "        logits['t4'] = conv(norm(t4), filters=1, strides=1)\n",
    "        logits['l5'] = conv(norm(l5), filters=1, strides=1)\n",
    "    else:\n",
    "        logits['t0'] = conv(t0, filters=1, strides=1)\n",
    "        logits['t1'] = conv(t1, filters=1, strides=1)\n",
    "        logits['t2'] = conv(t2, filters=1, strides=1)\n",
    "        logits['t3'] = conv(t3, filters=1, strides=1)\n",
    "        logits['t4'] = conv(t4, filters=1, strides=1)\n",
    "        logits['l5'] = conv(l5, filters=1, strides=1)\n",
    "\n",
    "    # --- Create model\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "    # --- Create deep supervision: pool op\n",
    "    mpool = lambda x : layers.MaxPooling3D(pool_size=2, strides=2, padding='valid')(x)\n",
    "\n",
    "    # --- Create deep supervision: subsampled ground-truth mask\n",
    "    y_true = {}\n",
    "    y_true['t0'] = tf.cast(inputs['y'] == 2, tf.float32)\n",
    "    y_true['t1'] = mpool(y_true['t0'])\n",
    "    y_true['t2'] = mpool(y_true['t1'])\n",
    "    y_true['t3'] = mpool(y_true['t2'])\n",
    "    y_true['t4'] = mpool(y_true['t3'])\n",
    "    y_true['l5'] = mpool(y_true['t4'])\n",
    "\n",
    "    # --- Create deep supervision: subsampled weight mask\n",
    "    w_samp = {}\n",
    "    w_samp['t0'] = y_true['t0'] * (class_weight - 1) + 1\n",
    "    w_samp['t1'] = mpool(w_samp['t0'])\n",
    "    w_samp['t2'] = mpool(w_samp['t1'])\n",
    "    w_samp['t3'] = mpool(w_samp['t2'])\n",
    "    w_samp['t4'] = mpool(w_samp['t3'])\n",
    "    w_samp['l5'] = mpool(w_samp['t4'])\n",
    "\n",
    "    # --- Create losses\n",
    "    loss_functions = {\n",
    "        'bce': create_bce,\n",
    "        'foc': create_foc,\n",
    "        'sft': create_sft}\n",
    "\n",
    "    for k in logits:\n",
    "        for l in losses:\n",
    "\n",
    "            loss = loss_functions[l](\n",
    "                y_true=y_true[k],\n",
    "                y_pred=logits[k])\n",
    "\n",
    "            model.add_loss(loss)\n",
    "            model.add_metric(loss, name='{}-{}'.format(k, l))\n",
    "\n",
    "    # --- Create DSC\n",
    "    dsc = create_dsc(y_true=y_true['t0'], y_pred=logits['t0'])\n",
    "    model.add_metric(dsc, name='dsc')\n",
    "\n",
    "    # --- Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a representative call to create the base VGG model with a combination of focal and Dice score loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(style='vgg', losses=['foc', 'sft'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the properties of the created model object, use the `model.summary()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print model summary\n",
    "model.summary(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory Data\n",
    "\n",
    "The following line of code will load all training data into RAM memory. This strategy can be effective for increasing speed of training for small to medium-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Optionally, to use Tensorboard, create the necessary Keras callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = callbacks.TensorBoard('./logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Once the model has been compiled and the data prepared (via a generator), training can be invoked using the `model.fit(...)` method. Ensure that both the training and validation data generators are used. In this particular example, we are defining arbitrary epochs of 100 steps each. Training will proceed for 200 epochs in total. Validation statistics will be assess every fifth epoch. As needed, tune these arugments as need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=100, \n",
    "    epochs=200,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=100,\n",
    "    validation_freq=5,\n",
    "    callbacks=[tensorboard_callback, lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Tensorboard\n",
    "\n",
    "After running several iterations, start Tensorboard using the following cells. After Tensorboard has registered the first several checkpoints, subsequent data will be updated automatically (asynchronously) and model training can be resumed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "model.save('./model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model('./model.hdf5', compile=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
