{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will explore how to create a simple convolutional neural network (CNN) for classification of brain tumors from MRI.\n",
    "\n",
    "## Workshop Links\n",
    "\n",
    "Use the following link to access materials from this workshop: https://github.com/peterchang77/dl_tutor/tree/master/workshops\n",
    "\n",
    "*Tutorials*\n",
    "\n",
    "* Introduction to Tensorflow 2.0 and Keras: https://bit.ly/2VSYaop\n",
    "* CNN for pneumonia classification: https://bit.ly/2D9ZBrX\n",
    "* CNN for pneumonia segmentation: https://bit.ly/2VQMWk9\n",
    "* CNN for brain tumor classification: https://bit.ly/44eekdt (**current tutorial**)\n",
    "* CNN for brain tumor segmentation: https://bit.ly/449i5Ro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Environment\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install Jarvis library\n",
    "%pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, models, layers, metrics\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of brain tumor MRI exams derived from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/. Each single 2D slice will consist of one of four different sequences (T2, FLAIR, T1 pre-contrast and T1 post-contrast). In this exercise, we will use this dataset to derive a model for slice-by-slice tumor detection. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/mr_brats_2020`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='mr/brats-2020-mip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality. As needed, pass any custom configurations (e.g. batch size, normalization parameters, etc) into the optional `configs` dictionary argument. \n",
    "\n",
    "Additionally, we will re-use this dataset for different tasks (e.g., tumor detection, survival prediction, etc). To specificy the correct Generator template file, pass a designated `keyword` string. In this tutorial, we will be using brain MRI volumes that have been preprocessed using a *mean intensity projection* (MIP) algorithm to subsample the original 155-slice inputs to 40-50 slices, facilitating ease of algorithm training within the Google Colab platform. In addition we will be performing global (entire-slice) tumor prediction. To select the correct Client template for this task, use the keyword string `mip*glb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*glb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, each iteration yields two variables, `xs` and `ys`, each representing a dictionary of model input(s) and output(s). In the current example, there is just a single input and output. Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('ys shape: {}'.format(ys['tumor'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wD5ejoTbx1_0"
   },
   "source": [
    "### BRATS Data\n",
    "\n",
    "Let us get to know this data a little bit more. The input images in the variable `dat` are matrices of shape `1 x 240 x 240 x 4`. Note that even though the images here are 2D in shape, the full matrix is a 3D tensor `(z, y, x)` where `z = 1` in this implementation. Note that although the 3rd z-axis dimension is redundant here (for a single slice input), many of our more complex models and architectures will commonly require a full 3D tensor. Because of this, we will directly use 3D convolutions throughout the tutorial materials for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1526486098306,
     "user": {
      "displayName": "Peter Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108798638501675293926"
     },
     "user_tz": 420
    },
    "id": "rrziy2-2x1_2",
    "outputId": "a250e47f-4f32-456c-bbfe-a8a087b43c4e"
   },
   "outputs": [],
   "source": [
    "print(xs['dat'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hx31sEGhx1_-"
   },
   "source": [
    "The four channels represent four different input MRI sequences. Each sequence is used to evaluate for a different tissue quality. T2 and FLAIR are used to evaluate edema (fluid) that results from brain injury. T1 images are used to evaluate anatomy and breakdown of the blood-brain-barrier through contrast leakge.  \n",
    "\n",
    "```\n",
    "dat[..., 0] = FLAIR\n",
    "dat[..., 1] = T1 precontrast\n",
    "dat[..., 2] = T1 postcontrast\n",
    "dat[..., 3] = T2\n",
    "```\n",
    "\n",
    "To visualize these different modalities run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1069
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1526486099897,
     "user": {
      "displayName": "Peter Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108798638501675293926"
     },
     "user_tz": 420
    },
    "id": "pOKYq_VPx2AC",
    "outputId": "3d85c3fb-4050-443d-e93a-4768fc3b4897"
   },
   "outputs": [],
   "source": [
    "imshow(xs['dat'][..., 0].astype('float32'), title='FLAIR')\n",
    "imshow(xs['dat'][..., 1].astype('float32'), title='T1 precontrast')\n",
    "imshow(xs['dat'][..., 2].astype('float32'), title='T1 postcontrast')\n",
    "imshow(xs['dat'][..., 3].astype('float32'), title='T2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inputs\n",
    "\n",
    "For every input in `xs`, a corresponding `Input(...)` variable should be created for model development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model inputs\n",
    "inputs = {}\n",
    "inputs['dat'] = Input(shape=(1, 240, 240, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Operations\n",
    "\n",
    "In this tutorial, a CNN will be created using 3D convolutional operations. A convolutional operation is defined by the following minimum specifications:\n",
    "\n",
    "* filter / channel depth\n",
    "* kernel size\n",
    "* strides\n",
    "* padding\n",
    "\n",
    "For a good review of convolutional math, see: http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n",
    "\n",
    "To instantiate a convolutional layer in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define regular convolution\n",
    "l1 = layers.Conv3D(\n",
    "    filters=16, \n",
    "    kernel_size=(1, 3, 3), \n",
    "    strides=(1, 1, 1), \n",
    "    padding='same')(inputs['dat'])\n",
    "\n",
    "# --- Define strided convolution\n",
    "l1 = layers.Conv3D(\n",
    "    filters=16, \n",
    "    kernel_size=(1, 3, 3), \n",
    "    strides=(1, 2, 2), \n",
    "    padding='same')(inputs['dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reuse identical function arguments, consider maintaining a `kwargs` dictionary and pass using the `**` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# ---- Define stack of convolutions\n",
    "l1 = layers.Conv3D(filters=16, strides=(1, 1, 1), **kwargs)(inputs['dat'])\n",
    "l2 = layers.Conv3D(filters=32, strides=(1, 1, 1), **kwargs)(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks\n",
    "\n",
    "In addition to the requisite convolutional operationss and activation functions, batch normalization is almost universally used in modern CNN architectures. Thus at minimum, a common baseline *block* pattern of operations can be defined as:\n",
    "\n",
    "* convolutional operation\n",
    "* batch normalization\n",
    "* activation function (e.g. ReLU)\n",
    "\n",
    "Let us define a block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define block\n",
    "c1 = layers.Conv3D(filters=16, **kwargs)(inputs['dat'])\n",
    "n1 = layers.BatchNormalization()(c1)\n",
    "r1 = layers.ReLU()(n1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course of buildling CNNs, we will be writing **many** blocks over time. Thus for brevity, let us use lambda functions to define modular, reusable components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let us rewrite a block using lambda shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define block\n",
    "b1 = relu(norm(conv(inputs['dat'], 16, (1, 1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the two most common **block patterns** will be regular convolutional block and a strided convolutional block (for subsampling). Let us then create two more high-level lambda functions for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see how easy it is to create series of alternating stride-1 and stride-2 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define series of blocks\n",
    "l1 = conv1(16, inputs['dat'])\n",
    "l2 = conv1(24, conv2(24, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "l5 = conv1(64, conv2(64, l4))\n",
    "l6 = conv1(80, conv2(80, l5))\n",
    "l7 = conv1(96, conv2(96, l6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a series of convolutional block operations, a CNN must transition to MLP type operations (e.g. matrix multiplications). To convert 3D (or 4D) feature maps into vectors, consider one of the following approaches:\n",
    "\n",
    "* serial convolutions (with stride > 1 or VALID type padding)\n",
    "* global pool operations (mean or max)\n",
    "* reshape / flatten operation\n",
    "\n",
    "Of these, the reshape / flatten operation is perhaps the most simple and easiest to use, and requires no additional learnable parameters. The following line of code implements the reshape / flatten operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = layers.Reshape([1, 1, 1, -1])(l7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now that the intermediate layer is defined as a vector, a number of standard MLP type operations may be performed, including creation of an arbitrary (optional) number of hidden layers. Once ready, the final layer should be implemented using a standard matrix multiplication operation (`layers.Dense(...)` object) that yields a vector of logit scores **without** any activation function applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final logit scores\n",
    "logits = {}\n",
    "logits['tumor'] = layers.Conv3D(filters=2, kernel_size=1, name='tumor')(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Putting everything together, use the following cell to create and compile the convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=2e-4), \n",
    "    loss={'tumor': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
    "    metrics={'tumor': 'sparse_categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "### In-Memory Data\n",
    "\n",
    "The following line of code will load all training data into RAM memory. This strategy can be effective for increasing speed of training for small to medium-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Once the model has been compiled and the data prepared (via a generator), training can be invoked using the `model.fit(...)` method. Ensure that both the training and validation data generators are used. In this particular example, we are defining arbitrary epochs of 100 steps each. Training will proceed for 8 epochs in total. Validation statistics will be assess every fourth epoch. As needed, tune these arugments as need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=100, \n",
    "    epochs=8,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=100,\n",
    "    validation_freq=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `model.predict(...)` to obtain logit scores\n",
    "* use `np.argmax(...)` to obtain prediction\n",
    "* compare prediction with ground-truth\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, we will create new **test mode** data generators, which will simply load each example individually once for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to loop through the test set generator and run model prediction on each example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test model\n",
    "pred = []\n",
    "true = []\n",
    "\n",
    "for x, y in test_valid:\n",
    "    \n",
    "    x['dat'] = x['dat'].reshape(-1, 1, 240, 240, 4)\n",
    "    logits = model.predict(x)\n",
    "    \n",
    "    if type(logits) is dict:\n",
    "        logits = logits['tumor']\n",
    "        \n",
    "    argmax = np.argmax(logits[0], axis=-1)\n",
    "    \n",
    "    pred.append(np.squeeze(argmax))\n",
    "    true.append(np.squeeze(y['tumor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to calculate validataion cohort performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate accuracy\n",
    "pred = np.array(pred)\n",
    "true = np.array(true)\n",
    "\n",
    "correct = np.sum(pred == true)\n",
    "\n",
    "print('Accuracy: {:0.5f}'.format(correct / pred.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "model.save('./cnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model('./cnn.hdf5', compile=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
